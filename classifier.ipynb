{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc718c9e-ceb9-4ddd-ae5e-a78d3dbbe1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6382/2436244604.py\", line 1, in <module>\n",
      "    from torchtext.datasets import AG_NEWS\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torchtext/__init__.py\", line 3, in <module>\n",
      "    from torch.hub import _get_torch_home\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/valentin/.pyenv/versions/3.12.7/envs/nlpworkshop/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "ag_news = load_dataset('ag_news')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['text'],        # Tokenize the 'text' field\n",
    "        padding='max_length',    # Pad to the maximum length (512 tokens by default)\n",
    "        truncation=True,         # Truncate sentences longer than the max length\n",
    "        max_length=512           # Set the maximum length to 512 tokens\n",
    ")\n",
    "\n",
    "tokenized_datasets = ag_news.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original 'text' column since we have tokenized data now\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "\n",
    "# Set the dataset format to PyTorch tensors\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Step 4: Create DataLoader for batching\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45f22d52-9052-4103-bad9-e8b2cafc3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, 4)\n",
    "        self.linear = nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        #x = self.linear(x)\n",
    "        x = torch.sum(x, dim=1, keepdim=False)\n",
    "        #x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee7e4bd-a157-4e10-a3a4-19f3df02eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100, Loss: 0.23083771459758282\n",
      "Batch 200, Loss: 0.22770953945815564\n",
      "Batch 300, Loss: 0.22654648818075657\n",
      "Batch 400, Loss: 0.2273319841362536\n",
      "Epoch 1 finished with average loss: 0.22799715388621858, and an average accuracy of 0.9364339019189766\n",
      "Batch 100, Loss: 0.24218751504906078\n",
      "Batch 200, Loss: 0.23278089880887698\n",
      "Batch 300, Loss: 0.2299418532347946\n",
      "Batch 400, Loss: 0.22574243416981912\n",
      "Epoch 2 finished with average loss: 0.22501130437013875, and an average accuracy of 0.9392051495776524\n",
      "Batch 100, Loss: 0.22957064144662592\n",
      "Batch 200, Loss: 0.22498534176673282\n",
      "Batch 300, Loss: 0.22713624179479672\n",
      "Batch 400, Loss: 0.22403307803062492\n",
      "Epoch 3 finished with average loss: 0.22133283749265453, and an average accuracy of 0.9403909846828238\n",
      "Batch 100, Loss: 0.22149594154805372\n",
      "Batch 200, Loss: 0.21778645621910905\n",
      "Batch 300, Loss: 0.2168228138630603\n",
      "Batch 400, Loss: 0.2160601931918423\n",
      "Epoch 4 finished with average loss: 0.2176468176084931, and an average accuracy of 0.9407932998962676\n",
      "Batch 100, Loss: 0.2183247962121548\n",
      "Batch 200, Loss: 0.21299872545025658\n",
      "Batch 300, Loss: 0.21374456900770075\n",
      "Batch 400, Loss: 0.21624729026790415\n",
      "Epoch 5 finished with average loss: 0.21391815287672003, and an average accuracy of 0.9414799030914632\n",
      "Batch 100, Loss: 0.2120421430809103\n",
      "Batch 200, Loss: 0.20997596414599826\n",
      "Batch 300, Loss: 0.21363017763835862\n",
      "Batch 400, Loss: 0.21086329889403785\n",
      "Epoch 6 finished with average loss: 0.21040245451298523, and an average accuracy of 0.9424919392034644\n",
      "Batch 100, Loss: 0.21285957086163435\n",
      "Batch 200, Loss: 0.21080650115051702\n",
      "Batch 300, Loss: 0.20883339249616753\n",
      "Batch 400, Loss: 0.20579959782604793\n",
      "Epoch 7 finished with average loss: 0.20675557944582654, and an average accuracy of 0.9434408143693038\n",
      "Batch 100, Loss: 0.2012848936114119\n",
      "Batch 200, Loss: 0.2008599085034632\n",
      "Batch 300, Loss: 0.2038983137936528\n",
      "Batch 400, Loss: 0.20434577167266127\n",
      "Epoch 8 finished with average loss: 0.2036650606266176, and an average accuracy of 0.9435872050057625\n",
      "Batch 100, Loss: 0.2006891607277309\n",
      "Batch 200, Loss: 0.20096733379029807\n",
      "Batch 300, Loss: 0.19899028258299548\n",
      "Batch 400, Loss: 0.20086851383951096\n",
      "Epoch 9 finished with average loss: 0.2003299540978741, and an average accuracy of 0.9447785485892092\n",
      "Batch 100, Loss: 0.19913626717696795\n",
      "Batch 200, Loss: 0.19261161861156528\n",
      "Batch 300, Loss: 0.19538966750823858\n",
      "Batch 400, Loss: 0.19689539205821902\n",
      "Epoch 10 finished with average loss: 0.19671807435417607, and an average accuracy of 0.9455390178719031\n",
      "Batch 100, Loss: 0.1919675233957434\n",
      "Batch 200, Loss: 0.18818359769867185\n",
      "Batch 300, Loss: 0.19272898721189183\n",
      "Batch 400, Loss: 0.19388844810419442\n",
      "Epoch 11 finished with average loss: 0.19406137721873348, and an average accuracy of 0.9457710719997269\n",
      "Batch 100, Loss: 0.18610212364367018\n",
      "Batch 200, Loss: 0.18926806878830674\n",
      "Batch 300, Loss: 0.1909945220287355\n",
      "Batch 400, Loss: 0.19064849369493964\n",
      "Epoch 12 finished with average loss: 0.19102028382648742, and an average accuracy of 0.9464989566211792\n",
      "Batch 100, Loss: 0.1810320229453673\n",
      "Batch 200, Loss: 0.18780857218418667\n",
      "Batch 300, Loss: 0.18676437341911362\n",
      "Batch 400, Loss: 0.18732613506721532\n",
      "Epoch 13 finished with average loss: 0.18812162874694804, and an average accuracy of 0.9469474924448213\n",
      "Batch 100, Loss: 0.1881708341537472\n",
      "Batch 200, Loss: 0.1881519424765585\n",
      "Batch 300, Loss: 0.1846498955686409\n",
      "Batch 400, Loss: 0.18684874613952032\n",
      "Epoch 14 finished with average loss: 0.1853628844308395, and an average accuracy of 0.9479451394650564\n",
      "Batch 100, Loss: 0.18098592104046296\n",
      "Batch 200, Loss: 0.18391894475384926\n",
      "Batch 300, Loss: 0.18054918562878752\n",
      "Batch 400, Loss: 0.18301246906079044\n",
      "Epoch 15 finished with average loss: 0.18263383020984583, and an average accuracy of 0.9487634979874877\n",
      "Batch 100, Loss: 0.1873783039112913\n",
      "Batch 200, Loss: 0.1794610376189166\n",
      "Batch 300, Loss: 0.1779528291957255\n",
      "Batch 400, Loss: 0.1789563749683407\n",
      "Epoch 16 finished with average loss: 0.18016661606278983, and an average accuracy of 0.9492372133930082\n",
      "Batch 100, Loss: 0.17870221647392184\n",
      "Batch 200, Loss: 0.1757555118340774\n",
      "Batch 300, Loss: 0.17723734588348358\n",
      "Batch 400, Loss: 0.17606797958480933\n",
      "Epoch 17 finished with average loss: 0.17713131819721661, and an average accuracy of 0.9501793881593312\n",
      "Batch 100, Loss: 0.17724820458040205\n",
      "Batch 200, Loss: 0.17362499118171915\n",
      "Batch 300, Loss: 0.1748429215224106\n",
      "Batch 400, Loss: 0.17427437335413087\n",
      "Epoch 18 finished with average loss: 0.17459967141603347, and an average accuracy of 0.9504395926897498\n",
      "Batch 100, Loss: 0.17021794261090506\n",
      "Batch 200, Loss: 0.17255240109588976\n",
      "Batch 300, Loss: 0.17015892200686053\n",
      "Batch 400, Loss: 0.17236232698349294\n",
      "Epoch 19 finished with average loss: 0.17201186171992194, and an average accuracy of 0.9511619847392105\n",
      "Batch 100, Loss: 0.17128507146754102\n",
      "Batch 200, Loss: 0.17150175613422314\n",
      "Batch 300, Loss: 0.17064501353276518\n",
      "Batch 400, Loss: 0.16999787301326352\n",
      "Epoch 20 finished with average loss: 0.16942113752731602, and an average accuracy of 0.9516216140399556\n",
      "Batch 100, Loss: 0.16689470471099704\n",
      "Batch 200, Loss: 0.16557219707049772\n",
      "Batch 300, Loss: 0.16466889677331079\n",
      "Batch 400, Loss: 0.16694748062927445\n",
      "Epoch 21 finished with average loss: 0.16718897428648066, and an average accuracy of 0.9522722475779103\n",
      "Batch 100, Loss: 0.16553573148514197\n",
      "Batch 200, Loss: 0.1650962412832447\n",
      "Batch 300, Loss: 0.16328100011436597\n",
      "Batch 400, Loss: 0.1652663885294531\n",
      "Epoch 22 finished with average loss: 0.1648252069008913, and an average accuracy of 0.9529205120772806\n",
      "Batch 100, Loss: 0.15925675301159678\n",
      "Batch 200, Loss: 0.16058948229206232\n",
      "Batch 300, Loss: 0.15906863462893586\n",
      "Batch 400, Loss: 0.160814711410317\n",
      "Epoch 23 finished with average loss: 0.1625801063007664, and an average accuracy of 0.9535465611487078\n",
      "Batch 100, Loss: 0.15598627842856103\n",
      "Batch 200, Loss: 0.15918317019886102\n",
      "Batch 300, Loss: 0.15917677946353176\n",
      "Batch 400, Loss: 0.1594928618052171\n",
      "Epoch 24 finished with average loss: 0.1601651974083659, and an average accuracy of 0.9542475228738068\n",
      "Batch 100, Loss: 0.15658446702367235\n",
      "Batch 200, Loss: 0.15675002319368805\n",
      "Batch 300, Loss: 0.15696861022216138\n",
      "Batch 400, Loss: 0.15717164792867047\n",
      "Epoch 25 finished with average loss: 0.15769872838088478, and an average accuracy of 0.9549486443273785\n",
      "Batch 100, Loss: 0.15806417152522292\n",
      "Batch 200, Loss: 0.15724458638697011\n",
      "Batch 300, Loss: 0.15753152904490061\n",
      "Batch 400, Loss: 0.15714138055279425\n",
      "Epoch 26 finished with average loss: 0.15571330686354834, and an average accuracy of 0.9553638075216646\n",
      "Batch 100, Loss: 0.1531335837592626\n",
      "Batch 200, Loss: 0.1529634397948754\n",
      "Batch 300, Loss: 0.15143589753077905\n",
      "Batch 400, Loss: 0.15300767192182302\n",
      "Epoch 27 finished with average loss: 0.15378458587864366, and an average accuracy of 0.9558977417715459\n",
      "Batch 100, Loss: 0.15688996409759315\n",
      "Batch 200, Loss: 0.1504978473565747\n",
      "Batch 300, Loss: 0.14963935763185351\n",
      "Batch 400, Loss: 0.1505534190352643\n",
      "Epoch 28 finished with average loss: 0.15148379663570305, and an average accuracy of 0.9563458640549499\n",
      "Batch 100, Loss: 0.1520811726162775\n",
      "Batch 200, Loss: 0.14674401465852774\n",
      "Batch 300, Loss: 0.14743506457689548\n",
      "Batch 400, Loss: 0.14764300762700397\n",
      "Epoch 29 finished with average loss: 0.1494421713167489, and an average accuracy of 0.9570991960498683\n",
      "Batch 100, Loss: 0.14344475948821908\n",
      "Batch 200, Loss: 0.1470097734481663\n",
      "Batch 300, Loss: 0.1488561853568137\n",
      "Batch 400, Loss: 0.14782488692927212\n",
      "Epoch 30 finished with average loss: 0.14771559804616835, and an average accuracy of 0.9574284053576045\n",
      "Batch 100, Loss: 0.14831605609207735\n",
      "Batch 200, Loss: 0.14411208633184128\n",
      "Batch 300, Loss: 0.14656809360931114\n",
      "Batch 400, Loss: 0.14669145063422467\n",
      "Epoch 31 finished with average loss: 0.14581620219737354, and an average accuracy of 0.957687302925425\n",
      "Batch 100, Loss: 0.14437182220552267\n",
      "Batch 200, Loss: 0.14233275385068805\n",
      "Batch 300, Loss: 0.1423333703941236\n",
      "Batch 400, Loss: 0.14409729371526317\n",
      "Epoch 32 finished with average loss: 0.14368079225208272, and an average accuracy of 0.958379152920239\n",
      "Batch 100, Loss: 0.1462765569979932\n",
      "Batch 200, Loss: 0.14389792349164615\n",
      "Batch 300, Loss: 0.14026320739322193\n",
      "Batch 400, Loss: 0.14066306096489195\n",
      "Epoch 33 finished with average loss: 0.14185576078119871, and an average accuracy of 0.9592718194447482\n",
      "Batch 100, Loss: 0.14158094688622383\n",
      "Batch 200, Loss: 0.14167495289503648\n",
      "Batch 300, Loss: 0.14007650058507332\n",
      "Batch 400, Loss: 0.14032731892384148\n",
      "Epoch 34 finished with average loss: 0.14035349938018876, and an average accuracy of 0.959023856047146\n",
      "Batch 100, Loss: 0.13484992890087205\n",
      "Batch 200, Loss: 0.13592152332155624\n",
      "Batch 300, Loss: 0.1368138220981672\n",
      "Batch 400, Loss: 0.13748157946958503\n",
      "Epoch 35 finished with average loss: 0.13821147052310895, and an average accuracy of 0.9600533335594466\n",
      "Batch 100, Loss: 0.13582082555123795\n",
      "Batch 200, Loss: 0.13597835350994583\n",
      "Batch 300, Loss: 0.13552071149292466\n",
      "Batch 400, Loss: 0.13573637005048153\n",
      "Epoch 36 finished with average loss: 0.13655026228533929, and an average accuracy of 0.960605235430475\n",
      "Batch 100, Loss: 0.12871373876644243\n",
      "Batch 200, Loss: 0.1317378573291653\n",
      "Batch 300, Loss: 0.13282701369279976\n",
      "Batch 400, Loss: 0.134248752323198\n",
      "Epoch 37 finished with average loss: 0.13530510205611462, and an average accuracy of 0.9608979233875561\n",
      "Batch 100, Loss: 0.13779627100637407\n",
      "Batch 200, Loss: 0.13584880134592875\n",
      "Batch 300, Loss: 0.13391428814914771\n",
      "Batch 400, Loss: 0.13205279628065936\n",
      "Epoch 38 finished with average loss: 0.13316438345726714, and an average accuracy of 0.9616703580456025\n",
      "Batch 100, Loss: 0.13265432327972018\n",
      "Batch 200, Loss: 0.13101501507677715\n",
      "Batch 300, Loss: 0.13110645400976528\n",
      "Batch 400, Loss: 0.13093418014232358\n",
      "Epoch 39 finished with average loss: 0.13150599631448157, and an average accuracy of 0.9620717918081996\n",
      "Batch 100, Loss: 0.12963208541599705\n",
      "Batch 200, Loss: 0.13384879559441307\n",
      "Batch 300, Loss: 0.1301153294380433\n",
      "Batch 400, Loss: 0.13051600567146587\n",
      "Epoch 40 finished with average loss: 0.13010715502671638, and an average accuracy of 0.9626223545667552\n",
      "Batch 100, Loss: 0.13024705512406154\n",
      "Batch 200, Loss: 0.12666186857015752\n",
      "Batch 300, Loss: 0.1265351446406104\n",
      "Batch 400, Loss: 0.12772107006505562\n",
      "Epoch 41 finished with average loss: 0.12844520765556058, and an average accuracy of 0.9628650663210379\n",
      "Batch 100, Loss: 0.12746786344379316\n",
      "Batch 200, Loss: 0.12692177526444262\n",
      "Batch 300, Loss: 0.12700995457530065\n",
      "Batch 400, Loss: 0.1253798823606971\n",
      "Epoch 42 finished with average loss: 0.12702856822791667, and an average accuracy of 0.9634097380589645\n",
      "Batch 100, Loss: 0.12890335625548988\n",
      "Batch 200, Loss: 0.1287356692485006\n",
      "Batch 300, Loss: 0.12663986297494778\n",
      "Batch 400, Loss: 0.1260911352991805\n",
      "Epoch 43 finished with average loss: 0.12573347677266045, and an average accuracy of 0.9638245676717676\n",
      "Batch 100, Loss: 0.12206614714756364\n",
      "Batch 200, Loss: 0.12044864004798111\n",
      "Batch 300, Loss: 0.12251025067234951\n",
      "Batch 400, Loss: 0.12270987094540088\n",
      "Epoch 44 finished with average loss: 0.12397872710733092, and an average accuracy of 0.9641974759793996\n",
      "Batch 100, Loss: 0.12397149887477443\n",
      "Batch 200, Loss: 0.1227976328720405\n",
      "Batch 300, Loss: 0.12284782480628981\n",
      "Batch 400, Loss: 0.12274024461531412\n",
      "Epoch 45 finished with average loss: 0.12242600390195398, and an average accuracy of 0.9648062801548957\n",
      "Batch 100, Loss: 0.12785402969210144\n",
      "Batch 200, Loss: 0.12201747151433823\n",
      "Batch 300, Loss: 0.12362379456787902\n",
      "Batch 400, Loss: 0.12163495513781605\n",
      "Epoch 46 finished with average loss: 0.12144341057030375, and an average accuracy of 0.9650435634971319\n",
      "Batch 100, Loss: 0.1238694592302529\n",
      "Batch 200, Loss: 0.12091818972108205\n",
      "Batch 300, Loss: 0.12021558113762036\n",
      "Batch 400, Loss: 0.1196828701095156\n",
      "Epoch 47 finished with average loss: 0.11970222982273869, and an average accuracy of 0.9656132103343933\n",
      "Batch 100, Loss: 0.12136284234548672\n",
      "Batch 200, Loss: 0.12300322178093127\n",
      "Batch 300, Loss: 0.12179188365298861\n",
      "Batch 400, Loss: 0.1190043895286199\n",
      "Epoch 48 finished with average loss: 0.11841530496319444, and an average accuracy of 0.9656449642011393\n",
      "Batch 100, Loss: 0.12024562479500377\n",
      "Batch 200, Loss: 0.11804001330184143\n",
      "Batch 300, Loss: 0.11592397311559784\n",
      "Batch 400, Loss: 0.11714029621691605\n",
      "Epoch 49 finished with average loss: 0.1172219006363356, and an average accuracy of 0.9663280009897679\n",
      "Batch 100, Loss: 0.11372431484425446\n",
      "Batch 200, Loss: 0.1153374569928367\n",
      "Batch 300, Loss: 0.1158426341954681\n",
      "Batch 400, Loss: 0.11583927213540768\n",
      "Epoch 50 finished with average loss: 0.11599268169601416, and an average accuracy of 0.9661045772942212\n",
      "Batch 100, Loss: 0.11532120143636064\n",
      "Batch 200, Loss: 0.11554371232918896\n",
      "Batch 300, Loss: 0.11544982108478412\n",
      "Batch 400, Loss: 0.11464086667079229\n",
      "Epoch 51 finished with average loss: 0.11452700826312602, and an average accuracy of 0.9670119500581966\n",
      "Batch 100, Loss: 0.1181207838538076\n",
      "Batch 200, Loss: 0.1128836404473973\n",
      "Batch 300, Loss: 0.11457153010725264\n",
      "Batch 400, Loss: 0.11436220247938696\n",
      "Epoch 52 finished with average loss: 0.11340173421802859, and an average accuracy of 0.9673248300285533\n",
      "Batch 100, Loss: 0.10703879737243083\n",
      "Batch 200, Loss: 0.10961513508550837\n",
      "Batch 300, Loss: 0.11203082139586419\n",
      "Batch 400, Loss: 0.11254078193787492\n",
      "Epoch 53 finished with average loss: 0.11214798406303443, and an average accuracy of 0.9676447713149153\n",
      "Batch 100, Loss: 0.10806357276594047\n",
      "Batch 200, Loss: 0.10988861561271253\n",
      "Batch 300, Loss: 0.1097390724003494\n",
      "Batch 400, Loss: 0.11103367738650592\n",
      "Epoch 54 finished with average loss: 0.11093440245401251, and an average accuracy of 0.9680785558379138\n",
      "Batch 100, Loss: 0.10459132395345087\n",
      "Batch 200, Loss: 0.10722254488447365\n",
      "Batch 300, Loss: 0.10804125056799291\n",
      "Batch 400, Loss: 0.11026771870356827\n",
      "Epoch 55 finished with average loss: 0.10987524669902435, and an average accuracy of 0.9683376763806069\n",
      "Batch 100, Loss: 0.10924326300437619\n",
      "Batch 200, Loss: 0.10813477514219966\n",
      "Batch 300, Loss: 0.10862925400275446\n",
      "Batch 400, Loss: 0.10941897377325935\n",
      "Epoch 56 finished with average loss: 0.10887639118053294, and an average accuracy of 0.9687269104684731\n",
      "Batch 100, Loss: 0.10133231448807496\n",
      "Batch 200, Loss: 0.10598294298563489\n",
      "Batch 300, Loss: 0.10741360700023894\n",
      "Batch 400, Loss: 0.10764759853216056\n",
      "Epoch 57 finished with average loss: 0.10750054092036589, and an average accuracy of 0.9692691183236712\n",
      "Batch 100, Loss: 0.10370351089992394\n",
      "Batch 200, Loss: 0.10347299398336941\n",
      "Batch 300, Loss: 0.10402995075685538\n",
      "Batch 400, Loss: 0.10585876779372065\n",
      "Epoch 58 finished with average loss: 0.10647434858595103, and an average accuracy of 0.9696728374946489\n",
      "Batch 100, Loss: 0.10638042805027116\n",
      "Batch 200, Loss: 0.10556210417887145\n",
      "Batch 300, Loss: 0.10548813239131088\n",
      "Batch 400, Loss: 0.10500490626613583\n",
      "Epoch 59 finished with average loss: 0.10521200107291383, and an average accuracy of 0.9699152361495976\n",
      "Batch 100, Loss: 0.10772949473875315\n",
      "Batch 200, Loss: 0.10728892977320925\n",
      "Batch 300, Loss: 0.1072332362269943\n",
      "Batch 400, Loss: 0.10604162619900592\n",
      "Epoch 60 finished with average loss: 0.10456320032989451, and an average accuracy of 0.9701184226783575\n",
      "Batch 100, Loss: 0.10186933947210665\n",
      "Batch 200, Loss: 0.10345326860177098\n",
      "Batch 300, Loss: 0.1040533354674947\n",
      "Batch 400, Loss: 0.10331612645984975\n",
      "Epoch 61 finished with average loss: 0.10299004066569793, and an average accuracy of 0.9707907198067058\n",
      "Batch 100, Loss: 0.10397869313245846\n",
      "Batch 200, Loss: 0.10199347033220446\n",
      "Batch 300, Loss: 0.10232474290492082\n",
      "Batch 400, Loss: 0.1029164607965346\n",
      "Epoch 62 finished with average loss: 0.10216881929342392, and an average accuracy of 0.9708560081090405\n",
      "Batch 100, Loss: 0.09873294195517943\n",
      "Batch 200, Loss: 0.100643996402547\n",
      "Batch 300, Loss: 0.10050576578324055\n",
      "Batch 400, Loss: 0.10021874534521263\n",
      "Epoch 63 finished with average loss: 0.1011676863915822, and an average accuracy of 0.9710449355183561\n",
      "Batch 100, Loss: 0.10073584456104745\n",
      "Batch 200, Loss: 0.101326337326858\n",
      "Batch 300, Loss: 0.10221697443511324\n",
      "Batch 400, Loss: 0.10128304614712474\n",
      "Epoch 64 finished with average loss: 0.10040729327710349, and an average accuracy of 0.971392375484403\n",
      "Batch 100, Loss: 0.09956321092533191\n",
      "Batch 200, Loss: 0.0976646921142296\n",
      "Batch 300, Loss: 0.09820250088419226\n",
      "Batch 400, Loss: 0.09856018660177071\n",
      "Epoch 65 finished with average loss: 0.0992708181756373, and an average accuracy of 0.9716874037856811\n",
      "Batch 100, Loss: 0.09715974060239159\n",
      "Batch 200, Loss: 0.0980366580997495\n",
      "Batch 300, Loss: 0.0979000603698749\n",
      "Batch 400, Loss: 0.09855568404630265\n",
      "Epoch 66 finished with average loss: 0.09825703326433573, and an average accuracy of 0.9720267410883845\n",
      "Batch 100, Loss: 0.09680474532878097\n",
      "Batch 200, Loss: 0.09676463265958278\n",
      "Batch 300, Loss: 0.09669908505590656\n",
      "Batch 400, Loss: 0.09745577184976562\n",
      "Epoch 67 finished with average loss: 0.09720623637384378, and an average accuracy of 0.972202371338497\n",
      "Batch 100, Loss: 0.09716061411004723\n",
      "Batch 200, Loss: 0.09672465053176613\n",
      "Batch 300, Loss: 0.09579613047822139\n",
      "Batch 400, Loss: 0.09697125539544091\n",
      "Epoch 68 finished with average loss: 0.0964862967063866, and an average accuracy of 0.972513691090274\n",
      "Batch 100, Loss: 0.09599164802782723\n",
      "Batch 200, Loss: 0.0938541147495458\n",
      "Batch 300, Loss: 0.0936647391822702\n",
      "Batch 400, Loss: 0.09396673290109384\n",
      "Epoch 69 finished with average loss: 0.09538002724470654, and an average accuracy of 0.9731168113527155\n",
      "Batch 100, Loss: 0.0902787181402268\n",
      "Batch 200, Loss: 0.09624577671488237\n",
      "Batch 300, Loss: 0.09437826248639233\n",
      "Batch 400, Loss: 0.09448014982144093\n",
      "Epoch 70 finished with average loss: 0.09489063832890382, and an average accuracy of 0.9731569654826284\n",
      "Batch 100, Loss: 0.09059146530069226\n",
      "Batch 200, Loss: 0.09475401628333983\n",
      "Batch 300, Loss: 0.09417057426981566\n",
      "Batch 400, Loss: 0.09312794039742975\n",
      "Epoch 71 finished with average loss: 0.09387834019665041, and an average accuracy of 0.9733763785689039\n",
      "Batch 100, Loss: 0.096973528975964\n",
      "Batch 200, Loss: 0.09320355557015558\n",
      "Batch 300, Loss: 0.09267110678130401\n",
      "Batch 400, Loss: 0.09254989010337006\n",
      "Epoch 72 finished with average loss: 0.09282071608104187, and an average accuracy of 0.9738432643110921\n",
      "Batch 100, Loss: 0.09153563295481583\n",
      "Batch 200, Loss: 0.09086476797698925\n",
      "Batch 300, Loss: 0.09167040571877685\n",
      "Batch 400, Loss: 0.09267064681507145\n",
      "Epoch 73 finished with average loss: 0.09220571264122167, and an average accuracy of 0.9739025620418857\n",
      "Batch 100, Loss: 0.09102875460008722\n",
      "Batch 200, Loss: 0.08985187558491996\n",
      "Batch 300, Loss: 0.09111912132574433\n",
      "Batch 400, Loss: 0.0920326227044561\n",
      "Epoch 74 finished with average loss: 0.09141935264361002, and an average accuracy of 0.9741719892936217\n",
      "Batch 100, Loss: 0.09179451661609532\n",
      "Batch 200, Loss: 0.08975218075540127\n",
      "Batch 300, Loss: 0.08935608263579647\n",
      "Batch 400, Loss: 0.0896508958899799\n",
      "Epoch 75 finished with average loss: 0.0906056967991987, and an average accuracy of 0.9745862320311874\n",
      "Batch 100, Loss: 0.08771962205084005\n",
      "Batch 200, Loss: 0.08985217257470807\n",
      "Batch 300, Loss: 0.08900121868322902\n",
      "Batch 400, Loss: 0.08925223334260743\n",
      "Epoch 76 finished with average loss: 0.0898230257774633, and an average accuracy of 0.9748980605515946\n",
      "Batch 100, Loss: 0.08955745674237135\n",
      "Batch 200, Loss: 0.08922724297053876\n",
      "Batch 300, Loss: 0.08895471021388811\n",
      "Batch 400, Loss: 0.08969955920133621\n",
      "Epoch 77 finished with average loss: 0.08889204116648475, and an average accuracy of 0.9748348705981911\n",
      "Batch 100, Loss: 0.08395453920980156\n",
      "Batch 200, Loss: 0.08703032120327085\n",
      "Batch 300, Loss: 0.08829397764808555\n",
      "Batch 400, Loss: 0.08865899012056457\n",
      "Epoch 78 finished with average loss: 0.08836207746637542, and an average accuracy of 0.9751206944646728\n",
      "Batch 100, Loss: 0.08187263777694827\n",
      "Batch 200, Loss: 0.08457321663494768\n",
      "Batch 300, Loss: 0.08674213155554972\n",
      "Batch 400, Loss: 0.08679608895031542\n",
      "Epoch 79 finished with average loss: 0.08727833957939794, and an average accuracy of 0.9754988803009197\n",
      "Batch 100, Loss: 0.08441921162917908\n",
      "Batch 200, Loss: 0.08299182710952906\n",
      "Batch 300, Loss: 0.08444020503366886\n",
      "Batch 400, Loss: 0.08648283015828503\n",
      "Epoch 80 finished with average loss: 0.08675249584069955, and an average accuracy of 0.9757162378400162\n",
      "Batch 100, Loss: 0.08686522436440498\n",
      "Batch 200, Loss: 0.08641714640483514\n",
      "Batch 300, Loss: 0.08556541362504333\n",
      "Batch 400, Loss: 0.08581687452532477\n",
      "Epoch 81 finished with average loss: 0.08593761533530644, and an average accuracy of 0.9760304228596446\n",
      "Batch 100, Loss: 0.08719574890882097\n",
      "Batch 200, Loss: 0.08556965205744069\n",
      "Batch 300, Loss: 0.08718131754497793\n",
      "Batch 400, Loss: 0.08634865964725695\n",
      "Epoch 82 finished with average loss: 0.08523941274327325, and an average accuracy of 0.9762615254218756\n",
      "Batch 100, Loss: 0.0870382318134946\n",
      "Batch 200, Loss: 0.08515214815044614\n",
      "Batch 300, Loss: 0.0832660205864523\n",
      "Batch 400, Loss: 0.08439149166111753\n",
      "Epoch 83 finished with average loss: 0.08442252712835725, and an average accuracy of 0.9767561990591795\n",
      "Batch 100, Loss: 0.08775775315487283\n",
      "Batch 200, Loss: 0.08356224845287154\n",
      "Batch 300, Loss: 0.08456149123070604\n",
      "Batch 400, Loss: 0.08440517573213552\n",
      "Epoch 84 finished with average loss: 0.08372972088745893, and an average accuracy of 0.9771126198274184\n",
      "Batch 100, Loss: 0.08268405198332583\n",
      "Batch 200, Loss: 0.08305896564958948\n",
      "Batch 300, Loss: 0.08278269535312029\n",
      "Batch 400, Loss: 0.08342849294475446\n",
      "Epoch 85 finished with average loss: 0.08341036943331312, and an average accuracy of 0.9770911694097243\n",
      "Batch 100, Loss: 0.07827829515762477\n",
      "Batch 200, Loss: 0.08200308903220967\n",
      "Batch 300, Loss: 0.08188747193110436\n",
      "Batch 400, Loss: 0.08255868047060407\n",
      "Epoch 86 finished with average loss: 0.08230999388987352, and an average accuracy of 0.9772688066867301\n",
      "Batch 100, Loss: 0.08097819275197332\n",
      "Batch 200, Loss: 0.08107042595265777\n",
      "Batch 300, Loss: 0.08166618078628164\n",
      "Batch 400, Loss: 0.08230905410594479\n",
      "Epoch 87 finished with average loss: 0.08188419111329724, and an average accuracy of 0.9773774610306044\n",
      "Batch 100, Loss: 0.07768804565745178\n",
      "Batch 200, Loss: 0.08002467509695681\n",
      "Batch 300, Loss: 0.08255794193820021\n",
      "Batch 400, Loss: 0.08109859558921949\n",
      "Epoch 88 finished with average loss: 0.08118465714506669, and an average accuracy of 0.977941281011437\n",
      "Batch 100, Loss: 0.08190225930738969\n",
      "Batch 200, Loss: 0.0792904605988588\n",
      "Batch 300, Loss: 0.07988593970066482\n",
      "Batch 400, Loss: 0.07995990223680566\n",
      "Epoch 89 finished with average loss: 0.0806932535559522, and an average accuracy of 0.9778758520561722\n",
      "Batch 100, Loss: 0.07902843614348827\n",
      "Batch 200, Loss: 0.07915398935441703\n",
      "Batch 300, Loss: 0.0792699177105528\n",
      "Batch 400, Loss: 0.07944362192707524\n",
      "Epoch 90 finished with average loss: 0.07993545250021457, and an average accuracy of 0.9780339614827068\n",
      "Batch 100, Loss: 0.07757233264455428\n",
      "Batch 200, Loss: 0.07803273617555315\n",
      "Batch 300, Loss: 0.07885067967874881\n",
      "Batch 400, Loss: 0.07951913680548278\n",
      "Epoch 91 finished with average loss: 0.07940119755644964, and an average accuracy of 0.9782675075582431\n",
      "Batch 100, Loss: 0.07694155133042528\n",
      "Batch 200, Loss: 0.07662435551496244\n",
      "Batch 300, Loss: 0.0781761623133348\n",
      "Batch 400, Loss: 0.07796064724841166\n",
      "Epoch 92 finished with average loss: 0.07859985368872797, and an average accuracy of 0.9788177123473167\n",
      "Batch 100, Loss: 0.07582609193647158\n",
      "Batch 200, Loss: 0.07647234504234916\n",
      "Batch 300, Loss: 0.07797379497521206\n",
      "Batch 400, Loss: 0.07807130053701314\n",
      "Epoch 93 finished with average loss: 0.07805188989958964, and an average accuracy of 0.9787966751151684\n",
      "Batch 100, Loss: 0.07792952089223483\n",
      "Batch 200, Loss: 0.07716849240766574\n",
      "Batch 300, Loss: 0.0770750190399197\n",
      "Batch 400, Loss: 0.07707806899824048\n",
      "Epoch 94 finished with average loss: 0.07758186348224927, and an average accuracy of 0.9788521562013827\n",
      "Batch 100, Loss: 0.07908295761425746\n",
      "Batch 200, Loss: 0.07686304880122072\n",
      "Batch 300, Loss: 0.07598216533538188\n",
      "Batch 400, Loss: 0.07686019891780976\n",
      "Epoch 95 finished with average loss: 0.0769988727571263, and an average accuracy of 0.9790132997289297\n",
      "Batch 100, Loss: 0.07690391045138409\n",
      "Batch 200, Loss: 0.07521047784052874\n",
      "Batch 300, Loss: 0.07612675396129545\n",
      "Batch 400, Loss: 0.07570820779092652\n",
      "Epoch 96 finished with average loss: 0.07642290529538864, and an average accuracy of 0.9791441292815826\n",
      "Batch 100, Loss: 0.07730992087840335\n",
      "Batch 200, Loss: 0.07688166570722707\n",
      "Batch 300, Loss: 0.07753791564990843\n",
      "Batch 400, Loss: 0.07572885384719644\n",
      "Epoch 97 finished with average loss: 0.07592535962501039, and an average accuracy of 0.9793720645969046\n",
      "Batch 100, Loss: 0.07122915273654847\n",
      "Batch 200, Loss: 0.07438376038396552\n",
      "Batch 300, Loss: 0.07540416761652757\n",
      "Batch 400, Loss: 0.07515378593963482\n",
      "Epoch 98 finished with average loss: 0.07544621228324153, and an average accuracy of 0.979639075119965\n",
      "Batch 100, Loss: 0.0745600326134907\n",
      "Batch 200, Loss: 0.07496053690831596\n",
      "Batch 300, Loss: 0.07382760054615294\n",
      "Batch 400, Loss: 0.0740106056853917\n",
      "Epoch 99 finished with average loss: 0.07459484507310082, and an average accuracy of 0.9796396444387775\n",
      "Batch 100, Loss: 0.07366073885010359\n",
      "Batch 200, Loss: 0.07381717958026467\n",
      "Batch 300, Loss: 0.07387881541317264\n",
      "Batch 400, Loss: 0.07398409696530966\n",
      "Epoch 100 finished with average loss: 0.07430928490605156, and an average accuracy of 0.9799616961146526\n",
      "validation accuracy 0.8839015151515152\n",
      "validation loss tensor(0.4692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training complete and model saved!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "running_loss = 0\n",
    "running_accuracy = 0\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_idx = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Get inputs: input_ids, attention_mask, and labels\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Zero the gradients before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the model's predictions\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Get predicted classes by taking the argmax\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_predictions = (predicted_classes == labels).sum().item()\n",
    "        accuracy = correct_predictions / labels.size(0)  # Average accuracy\n",
    "        running_accuracy += accuracy\n",
    "\n",
    "        # Get the loss value\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step the optimizer (update the model's weights)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print loss every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Batch {batch_idx + 1}, Loss: {running_loss / (batch_idx + 1)}\")\n",
    "        batch_idx += 1\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = running_accuracy / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} finished with average loss: {epoch_loss}, and an average accuracy of {epoch_accuracy}\")\n",
    "    running_loss = epoch_loss\n",
    "    running_accuracy = epoch_accuracy\n",
    "\n",
    "validation_accuracy = 0\n",
    "validation_loss = 0\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "    \n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "    predicted_classes = torch.argmax(outputs, dim=1)\n",
    "    correct_predictions = (predicted_classes == labels).sum().item()\n",
    "    accuracy = correct_predictions / labels.size(0)\n",
    "    validation_accuracy += accuracy\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    validation_loss += loss\n",
    "\n",
    "validation_accuracy /= len(test_dataloader)\n",
    "validation_loss /= len(test_dataloader)\n",
    "print(\"validation accuracy\", validation_accuracy)\n",
    "print(\"validation loss\", validation_loss)\n",
    "\n",
    "# Step 8: Save the trained model    \n",
    "torch.save(model.state_dict(), './new_training_run')\n",
    "\n",
    "print(\"Training complete and model saved!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
